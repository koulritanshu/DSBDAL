{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e404eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RITANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RITANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perception_tagger: Package\n",
      "[nltk_data]     'averaged_perception_tagger' not found in index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RITANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perception_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d28d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the field of astronomy, telescopes serve as our windows to the cosmos, capturing photons emitted by celestial bodies millions of light-years away. through these lenses, we observe the intricate dance of galaxies, each containing billions of stars held together by gravity's unyielding embrace. amidst these stellar congregations, nebulae shimmer with the birth of new stars, their gaseous clouds sculpted by the forces of radiation and stellar winds. closer to home, our own solar system teems with activity, with planets orbiting the sun in a delicate balance of gravitational forces. yet, beyond the familiar realms of our own cosmic backyard, lie the mysteries of dark matter and dark energy, invisible forces that shape the very fabric of the universe, yet elude our understanding.\n",
      "meanwhile, aboard the international space station (iss), astronauts conduct experiments in microgravity, offering insights into the behavior of matter and biological processes in space. from this orbiting laboratory, scientists study the effects of long-duration space travel on the human body, essential for future missions to mars and beyond. outside the station, the earth hangs suspended against the backdrop of the cosmos, a reminder of the fragility of our existence in the vastness of space. yet, it is also a beacon of hope, a testament to humanity's ability to reach beyond the confines of our planet and explore the wonders of the universe.\n"
     ]
    }
   ],
   "source": [
    "with open('1.txt','r') as f:\n",
    "    file1 = f.read()\n",
    "\n",
    "file1 = file1.lower()\n",
    "\n",
    "with open('2.txt','r') as f:\n",
    "    file2 = f.read()\n",
    "    \n",
    "file2 = file2.lower()\n",
    "\n",
    "print(file1)\n",
    "print(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "047fe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenize on whole paragraph\n",
    "tokens_1 = nltk.word_tokenize(file1)\n",
    "tokens_2 = nltk.word_tokenize(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9f8d183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'field', 'of', 'astronomy', ',', 'telescopes', 'serve', 'as', 'our', 'windows', 'to', 'the', 'cosmos', ',', 'capturing', 'photons', 'emitted', 'by', 'celestial', 'bodies', 'millions', 'of', 'light-years', 'away', '.', 'through', 'these', 'lenses', ',', 'we', 'observe', 'the', 'intricate', 'dance', 'of', 'galaxies', ',', 'each', 'containing', 'billions', 'of', 'stars', 'held', 'together', 'by', 'gravity', \"'s\", 'unyielding', 'embrace', '.', 'amidst', 'these', 'stellar', 'congregations', ',', 'nebulae', 'shimmer', 'with', 'the', 'birth', 'of', 'new', 'stars', ',', 'their', 'gaseous', 'clouds', 'sculpted', 'by', 'the', 'forces', 'of', 'radiation', 'and', 'stellar', 'winds', '.', 'closer', 'to', 'home', ',', 'our', 'own', 'solar', 'system', 'teems', 'with', 'activity', ',', 'with', 'planets', 'orbiting', 'the', 'sun', 'in', 'a', 'delicate', 'balance', 'of', 'gravitational', 'forces', '.', 'yet', ',', 'beyond', 'the', 'familiar', 'realms', 'of', 'our', 'own', 'cosmic', 'backyard', ',', 'lie', 'the', 'mysteries', 'of', 'dark', 'matter', 'and', 'dark', 'energy', ',', 'invisible', 'forces', 'that', 'shape', 'the', 'very', 'fabric', 'of', 'the', 'universe', ',', 'yet', 'elude', 'our', 'understanding', '.']\n",
      "\n",
      "['meanwhile', ',', 'aboard', 'the', 'international', 'space', 'station', '(', 'iss', ')', ',', 'astronauts', 'conduct', 'experiments', 'in', 'microgravity', ',', 'offering', 'insights', 'into', 'the', 'behavior', 'of', 'matter', 'and', 'biological', 'processes', 'in', 'space', '.', 'from', 'this', 'orbiting', 'laboratory', ',', 'scientists', 'study', 'the', 'effects', 'of', 'long-duration', 'space', 'travel', 'on', 'the', 'human', 'body', ',', 'essential', 'for', 'future', 'missions', 'to', 'mars', 'and', 'beyond', '.', 'outside', 'the', 'station', ',', 'the', 'earth', 'hangs', 'suspended', 'against', 'the', 'backdrop', 'of', 'the', 'cosmos', ',', 'a', 'reminder', 'of', 'the', 'fragility', 'of', 'our', 'existence', 'in', 'the', 'vastness', 'of', 'space', '.', 'yet', ',', 'it', 'is', 'also', 'a', 'beacon', 'of', 'hope', ',', 'a', 'testament', 'to', 'humanity', \"'s\", 'ability', 'to', 'reach', 'beyond', 'the', 'confines', 'of', 'our', 'planet', 'and', 'explore', 'the', 'wonders', 'of', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_1)\n",
    "print()\n",
    "print(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d16ec2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the field of astronomy, telescopes serve as our windows to the cosmos, capturing photons emitted by celestial bodies millions of light-years away.', \"through these lenses, we observe the intricate dance of galaxies, each containing billions of stars held together by gravity's unyielding embrace.\", 'amidst these stellar congregations, nebulae shimmer with the birth of new stars, their gaseous clouds sculpted by the forces of radiation and stellar winds.', 'closer to home, our own solar system teems with activity, with planets orbiting the sun in a delicate balance of gravitational forces.', 'yet, beyond the familiar realms of our own cosmic backyard, lie the mysteries of dark matter and dark energy, invisible forces that shape the very fabric of the universe, yet elude our understanding.']\n",
      "['meanwhile, aboard the international space station (iss), astronauts conduct experiments in microgravity, offering insights into the behavior of matter and biological processes in space.', 'from this orbiting laboratory, scientists study the effects of long-duration space travel on the human body, essential for future missions to mars and beyond.', 'outside the station, the earth hangs suspended against the backdrop of the cosmos, a reminder of the fragility of our existence in the vastness of space.', \"yet, it is also a beacon of hope, a testament to humanity's ability to reach beyond the confines of our planet and explore the wonders of the universe.\"]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "tokens1_sent = nltk.sent_tokenize(file1)\n",
    "print(tokens1_sent)\n",
    "tokens2_sent = nltk.sent_tokenize(file2)\n",
    "print(tokens2_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5a09967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'the', 'field', 'of', 'astronomy', ',', 'telescopes', 'serve', 'as', 'our', 'windows', 'to', 'the', 'cosmos', ',', 'capturing', 'photons', 'emitted', 'by', 'celestial', 'bodies', 'millions', 'of', 'light-years', 'away', '.'], ['through', 'these', 'lenses', ',', 'we', 'observe', 'the', 'intricate', 'dance', 'of', 'galaxies', ',', 'each', 'containing', 'billions', 'of', 'stars', 'held', 'together', 'by', 'gravity', \"'s\", 'unyielding', 'embrace', '.'], ['amidst', 'these', 'stellar', 'congregations', ',', 'nebulae', 'shimmer', 'with', 'the', 'birth', 'of', 'new', 'stars', ',', 'their', 'gaseous', 'clouds', 'sculpted', 'by', 'the', 'forces', 'of', 'radiation', 'and', 'stellar', 'winds', '.'], ['closer', 'to', 'home', ',', 'our', 'own', 'solar', 'system', 'teems', 'with', 'activity', ',', 'with', 'planets', 'orbiting', 'the', 'sun', 'in', 'a', 'delicate', 'balance', 'of', 'gravitational', 'forces', '.'], ['yet', ',', 'beyond', 'the', 'familiar', 'realms', 'of', 'our', 'own', 'cosmic', 'backyard', ',', 'lie', 'the', 'mysteries', 'of', 'dark', 'matter', 'and', 'dark', 'energy', ',', 'invisible', 'forces', 'that', 'shape', 'the', 'very', 'fabric', 'of', 'the', 'universe', ',', 'yet', 'elude', 'our', 'understanding', '.']]\n",
      "\n",
      "[['meanwhile', ',', 'aboard', 'the', 'international', 'space', 'station', '(', 'iss', ')', ',', 'astronauts', 'conduct', 'experiments', 'in', 'microgravity', ',', 'offering', 'insights', 'into', 'the', 'behavior', 'of', 'matter', 'and', 'biological', 'processes', 'in', 'space', '.'], ['from', 'this', 'orbiting', 'laboratory', ',', 'scientists', 'study', 'the', 'effects', 'of', 'long-duration', 'space', 'travel', 'on', 'the', 'human', 'body', ',', 'essential', 'for', 'future', 'missions', 'to', 'mars', 'and', 'beyond', '.'], ['outside', 'the', 'station', ',', 'the', 'earth', 'hangs', 'suspended', 'against', 'the', 'backdrop', 'of', 'the', 'cosmos', ',', 'a', 'reminder', 'of', 'the', 'fragility', 'of', 'our', 'existence', 'in', 'the', 'vastness', 'of', 'space', '.'], ['yet', ',', 'it', 'is', 'also', 'a', 'beacon', 'of', 'hope', ',', 'a', 'testament', 'to', 'humanity', \"'s\", 'ability', 'to', 'reach', 'beyond', 'the', 'confines', 'of', 'our', 'planet', 'and', 'explore', 'the', 'wonders', 'of', 'the', 'universe', '.']]\n"
     ]
    }
   ],
   "source": [
    "# applying word tokenization on tokenized sentences\n",
    "word_tokens1 = []\n",
    "for i in tokens1_sent:\n",
    "    word_tokens1.append(nltk.word_tokenize(i))\n",
    "print(word_tokens1)\n",
    "word_tokens2 = []\n",
    "for i in tokens2_sent:\n",
    "    word_tokens2.append(nltk.word_tokenize(i))\n",
    "print()\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "114c2716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('astronomy', 'NN'), (',', ','), ('telescopes', 'VBZ'), ('serve', 'VBP'), ('as', 'IN'), ('our', 'PRP$'), ('windows', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('cosmos', 'NN'), (',', ','), ('capturing', 'VBG'), ('photons', 'NNS'), ('emitted', 'VBN'), ('by', 'IN'), ('celestial', 'JJ'), ('bodies', 'NNS'), ('millions', 'NNS'), ('of', 'IN'), ('light-years', 'JJ'), ('away', 'NN'), ('.', '.'), ('through', 'IN'), ('these', 'DT'), ('lenses', 'NNS'), (',', ','), ('we', 'PRP'), ('observe', 'VBP'), ('the', 'DT'), ('intricate', 'JJ'), ('dance', 'NN'), ('of', 'IN'), ('galaxies', 'NNS'), (',', ','), ('each', 'DT'), ('containing', 'VBG'), ('billions', 'NNS'), ('of', 'IN'), ('stars', 'NNS'), ('held', 'VBN'), ('together', 'RB'), ('by', 'IN'), ('gravity', 'NN'), (\"'s\", 'POS'), ('unyielding', 'JJ'), ('embrace', 'NN'), ('.', '.'), ('amidst', 'IN'), ('these', 'DT'), ('stellar', 'JJ'), ('congregations', 'NNS'), (',', ','), ('nebulae', 'JJ'), ('shimmer', 'NN'), ('with', 'IN'), ('the', 'DT'), ('birth', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('stars', 'NNS'), (',', ','), ('their', 'PRP$'), ('gaseous', 'JJ'), ('clouds', 'NNS'), ('sculpted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('forces', 'NNS'), ('of', 'IN'), ('radiation', 'NN'), ('and', 'CC'), ('stellar', 'JJ'), ('winds', 'NNS'), ('.', '.'), ('closer', 'JJR'), ('to', 'TO'), ('home', 'NN'), (',', ','), ('our', 'PRP$'), ('own', 'JJ'), ('solar', 'JJ'), ('system', 'NN'), ('teems', 'VBZ'), ('with', 'IN'), ('activity', 'NN'), (',', ','), ('with', 'IN'), ('planets', 'NNS'), ('orbiting', 'VBG'), ('the', 'DT'), ('sun', 'NN'), ('in', 'IN'), ('a', 'DT'), ('delicate', 'JJ'), ('balance', 'NN'), ('of', 'IN'), ('gravitational', 'JJ'), ('forces', 'NNS'), ('.', '.'), ('yet', 'RB'), (',', ','), ('beyond', 'IN'), ('the', 'DT'), ('familiar', 'JJ'), ('realms', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('own', 'JJ'), ('cosmic', 'JJ'), ('backyard', 'NN'), (',', ','), ('lie', 'VBZ'), ('the', 'DT'), ('mysteries', 'NNS'), ('of', 'IN'), ('dark', 'JJ'), ('matter', 'NN'), ('and', 'CC'), ('dark', 'JJ'), ('energy', 'NN'), (',', ','), ('invisible', 'JJ'), ('forces', 'NNS'), ('that', 'WDT'), ('shape', 'VBP'), ('the', 'DT'), ('very', 'RB'), ('fabric', 'NN'), ('of', 'IN'), ('the', 'DT'), ('universe', 'NN'), (',', ','), ('yet', 'RB'), ('elude', 'VB'), ('our', 'PRP$'), ('understanding', 'NN'), ('.', '.')]\n",
      "[('meanwhile', 'RB'), (',', ','), ('aboard', 'IN'), ('the', 'DT'), ('international', 'JJ'), ('space', 'NN'), ('station', 'NN'), ('(', '('), ('iss', 'JJ'), (')', ')'), (',', ','), ('astronauts', 'JJ'), ('conduct', 'NN'), ('experiments', 'NNS'), ('in', 'IN'), ('microgravity', 'NN'), (',', ','), ('offering', 'VBG'), ('insights', 'NNS'), ('into', 'IN'), ('the', 'DT'), ('behavior', 'NN'), ('of', 'IN'), ('matter', 'NN'), ('and', 'CC'), ('biological', 'JJ'), ('processes', 'NNS'), ('in', 'IN'), ('space', 'NN'), ('.', '.'), ('from', 'IN'), ('this', 'DT'), ('orbiting', 'VBG'), ('laboratory', 'NN'), (',', ','), ('scientists', 'NNS'), ('study', 'VBP'), ('the', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('long-duration', 'JJ'), ('space', 'NN'), ('travel', 'NN'), ('on', 'IN'), ('the', 'DT'), ('human', 'JJ'), ('body', 'NN'), (',', ','), ('essential', 'JJ'), ('for', 'IN'), ('future', 'JJ'), ('missions', 'NNS'), ('to', 'TO'), ('mars', 'NNS'), ('and', 'CC'), ('beyond', 'IN'), ('.', '.'), ('outside', 'IN'), ('the', 'DT'), ('station', 'NN'), (',', ','), ('the', 'DT'), ('earth', 'NN'), ('hangs', 'NNS'), ('suspended', 'VBN'), ('against', 'IN'), ('the', 'DT'), ('backdrop', 'NN'), ('of', 'IN'), ('the', 'DT'), ('cosmos', 'NN'), (',', ','), ('a', 'DT'), ('reminder', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fragility', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('existence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('vastness', 'NN'), ('of', 'IN'), ('space', 'NN'), ('.', '.'), ('yet', 'RB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('also', 'RB'), ('a', 'DT'), ('beacon', 'NN'), ('of', 'IN'), ('hope', 'NN'), (',', ','), ('a', 'DT'), ('testament', 'NN'), ('to', 'TO'), ('humanity', 'NN'), (\"'s\", 'POS'), ('ability', 'NN'), ('to', 'TO'), ('reach', 'VB'), ('beyond', 'IN'), ('the', 'DT'), ('confines', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('planet', 'NN'), ('and', 'CC'), ('explore', 'VB'), ('the', 'DT'), ('wonders', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS TAGGING\n",
    "pos = nltk.pos_tag(tokens_1)\n",
    "print(pos)\n",
    "pos2 = nltk.pos_tag(tokens_2)\n",
    "print(pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "276933d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['in', 'the', 'field', 'of', 'astronomy', ',', 'telescope', 'serve', 'a', 'our', 'window', 'to', 'the', 'cosmos', ',', 'capturing', 'photon', 'emitted', 'by', 'celestial', 'body', 'million', 'of', 'light-year', 'away', '.', 'through', 'these', 'lens', ',', 'we', 'observe', 'the', 'intricate', 'dance', 'of', 'galaxy', ',', 'each', 'containing', 'billion', 'of', 'star', 'held', 'together', 'by', 'gravity', \"'s\", 'unyielding', 'embrace', '.', 'amidst', 'these', 'stellar', 'congregation', ',', 'nebula', 'shimmer', 'with', 'the', 'birth', 'of', 'new', 'star', ',', 'their', 'gaseous', 'cloud', 'sculpted', 'by', 'the', 'force', 'of', 'radiation', 'and', 'stellar', 'wind', '.', 'closer', 'to', 'home', ',', 'our', 'own', 'solar', 'system', 'teems', 'with', 'activity', ',', 'with', 'planet', 'orbiting', 'the', 'sun', 'in', 'a', 'delicate', 'balance', 'of', 'gravitational', 'force', '.', 'yet', ',', 'beyond', 'the', 'familiar', 'realm', 'of', 'our', 'own', 'cosmic', 'backyard', ',', 'lie', 'the', 'mystery', 'of', 'dark', 'matter', 'and', 'dark', 'energy', ',', 'invisible', 'force', 'that', 'shape', 'the', 'very', 'fabric', 'of', 'the', 'universe', ',', 'yet', 'elude', 'our', 'understanding', '.']\n",
      "\n",
      "['meanwhile', ',', 'aboard', 'the', 'international', 'space', 'station', '(', 'i', ')', ',', 'astronaut', 'conduct', 'experiment', 'in', 'microgravity', ',', 'offering', 'insight', 'into', 'the', 'behavior', 'of', 'matter', 'and', 'biological', 'process', 'in', 'space', '.', 'from', 'this', 'orbiting', 'laboratory', ',', 'scientist', 'study', 'the', 'effect', 'of', 'long-duration', 'space', 'travel', 'on', 'the', 'human', 'body', ',', 'essential', 'for', 'future', 'mission', 'to', 'mar', 'and', 'beyond', '.', 'outside', 'the', 'station', ',', 'the', 'earth', 'hang', 'suspended', 'against', 'the', 'backdrop', 'of', 'the', 'cosmos', ',', 'a', 'reminder', 'of', 'the', 'fragility', 'of', 'our', 'existence', 'in', 'the', 'vastness', 'of', 'space', '.', 'yet', ',', 'it', 'is', 'also', 'a', 'beacon', 'of', 'hope', ',', 'a', 'testament', 'to', 'humanity', \"'s\", 'ability', 'to', 'reach', 'beyond', 'the', 'confines', 'of', 'our', 'planet', 'and', 'explore', 'the', 'wonder', 'of', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "lemmatized_tokens2 = []\n",
    "for i in tokens_1:\n",
    "#     print(i + ' --> ' + lemmatizer.lemmatize(i))\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "for i in tokens_2:\n",
    "    lemmatized_tokens2.append(lemmatizer.lemmatize(i))\n",
    "print()\n",
    "print(lemmatized_tokens)\n",
    "print()\n",
    "print(lemmatized_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "27d9f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['in', 'the', 'field', 'of', 'astronomi', ',', 'telescop', 'serv', 'as', 'our', 'window', 'to', 'the', 'cosmo', ',', 'captur', 'photon', 'emit', 'by', 'celesti', 'bodi', 'million', 'of', 'light-year', 'away', '.', 'through', 'these', 'lens', ',', 'we', 'observ', 'the', 'intric', 'danc', 'of', 'galaxi', ',', 'each', 'contain', 'billion', 'of', 'star', 'held', 'togeth', 'by', 'graviti', \"'s\", 'unyield', 'embrac', '.', 'amidst', 'these', 'stellar', 'congreg', ',', 'nebula', 'shimmer', 'with', 'the', 'birth', 'of', 'new', 'star', ',', 'their', 'gaseou', 'cloud', 'sculpt', 'by', 'the', 'forc', 'of', 'radiat', 'and', 'stellar', 'wind', '.', 'closer', 'to', 'home', ',', 'our', 'own', 'solar', 'system', 'teem', 'with', 'activ', ',', 'with', 'planet', 'orbit', 'the', 'sun', 'in', 'a', 'delic', 'balanc', 'of', 'gravit', 'forc', '.', 'yet', ',', 'beyond', 'the', 'familiar', 'realm', 'of', 'our', 'own', 'cosmic', 'backyard', ',', 'lie', 'the', 'mysteri', 'of', 'dark', 'matter', 'and', 'dark', 'energi', ',', 'invis', 'forc', 'that', 'shape', 'the', 'veri', 'fabric', 'of', 'the', 'univers', ',', 'yet', 'elud', 'our', 'understand', '.']\n",
      "\n",
      "['meanwhil', ',', 'aboard', 'the', 'intern', 'space', 'station', '(', 'iss', ')', ',', 'astronaut', 'conduct', 'experi', 'in', 'micrograv', ',', 'offer', 'insight', 'into', 'the', 'behavior', 'of', 'matter', 'and', 'biolog', 'process', 'in', 'space', '.', 'from', 'thi', 'orbit', 'laboratori', ',', 'scientist', 'studi', 'the', 'effect', 'of', 'long-dur', 'space', 'travel', 'on', 'the', 'human', 'bodi', ',', 'essenti', 'for', 'futur', 'mission', 'to', 'mar', 'and', 'beyond', '.', 'outsid', 'the', 'station', ',', 'the', 'earth', 'hang', 'suspend', 'against', 'the', 'backdrop', 'of', 'the', 'cosmo', ',', 'a', 'remind', 'of', 'the', 'fragil', 'of', 'our', 'exist', 'in', 'the', 'vast', 'of', 'space', '.', 'yet', ',', 'it', 'is', 'also', 'a', 'beacon', 'of', 'hope', ',', 'a', 'testament', 'to', 'human', \"'s\", 'abil', 'to', 'reach', 'beyond', 'the', 'confin', 'of', 'our', 'planet', 'and', 'explor', 'the', 'wonder', 'of', 'the', 'univers', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = []\n",
    "stemmed_tokens2 = []\n",
    "for i in tokens_1:\n",
    "#     print(i + ' --> ' + stemmer.stem(i))\n",
    "    stemmed_tokens.append(stemmer.stem(i))\n",
    "for i in tokens_2:\n",
    "    stemmed_tokens2.append(stemmer.stem(i))\n",
    "print()\n",
    "print(stemmed_tokens)\n",
    "print()\n",
    "print(stemmed_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de9bea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the filtered tokens: \n",
      "\n",
      "['field', 'astronomy', ',', 'telescope', 'serve', 'window', 'cosmos', ',', 'capturing', 'photon', 'emitted', 'celestial', 'body', 'million', 'light-year', 'away', '.', 'lens', ',', 'observe', 'intricate', 'dance', 'galaxy', ',', 'containing', 'billion', 'star', 'held', 'together', 'gravity', \"'s\", 'unyielding', 'embrace', '.', 'amidst', 'stellar', 'congregation', ',', 'nebula', 'shimmer', 'birth', 'new', 'star', ',', 'gaseous', 'cloud', 'sculpted', 'force', 'radiation', 'stellar', 'wind', '.', 'closer', 'home', ',', 'solar', 'system', 'teems', 'activity', ',', 'planet', 'orbiting', 'sun', 'delicate', 'balance', 'gravitational', 'force', '.', 'yet', ',', 'beyond', 'familiar', 'realm', 'cosmic', 'backyard', ',', 'lie', 'mystery', 'dark', 'matter', 'dark', 'energy', ',', 'invisible', 'force', 'shape', 'fabric', 'universe', ',', 'yet', 'elude', 'understanding', '.']\n",
      "\n",
      "Following are the removed tokens: \n",
      "\n",
      "['in', 'the', 'of', 'a', 'our', 'to', 'the', 'by', 'of', 'through', 'these', 'we', 'the', 'of', 'each', 'of', 'by', 'these', 'with', 'the', 'of', 'their', 'by', 'the', 'of', 'and', 'to', 'our', 'own', 'with', 'with', 'the', 'in', 'a', 'of', 'the', 'of', 'our', 'own', 'the', 'of', 'and', 'that', 'the', 'very', 'of', 'the', 'our']\n",
      "\n",
      "Following are the filtered tokens: \n",
      "\n",
      "['meanwhile', ',', 'aboard', 'international', 'space', 'station', '(', ')', ',', 'astronaut', 'conduct', 'experiment', 'microgravity', ',', 'offering', 'insight', 'behavior', 'matter', 'biological', 'process', 'space', '.', 'orbiting', 'laboratory', ',', 'scientist', 'study', 'effect', 'long-duration', 'space', 'travel', 'human', 'body', ',', 'essential', 'future', 'mission', 'mar', 'beyond', '.', 'outside', 'station', ',', 'earth', 'hang', 'suspended', 'backdrop', 'cosmos', ',', 'reminder', 'fragility', 'existence', 'vastness', 'space', '.', 'yet', ',', 'also', 'beacon', 'hope', ',', 'testament', 'humanity', \"'s\", 'ability', 'reach', 'beyond', 'confines', 'planet', 'explore', 'wonder', 'universe', '.']\n",
      "\n",
      "Following are the removed tokens: \n",
      "\n",
      "['the', 'i', 'in', 'into', 'the', 'of', 'and', 'in', 'from', 'this', 'the', 'of', 'on', 'the', 'for', 'to', 'and', 'the', 'the', 'against', 'the', 'of', 'the', 'a', 'of', 'the', 'of', 'our', 'in', 'the', 'of', 'it', 'is', 'a', 'of', 'a', 'to', 'to', 'the', 'of', 'our', 'and', 'the', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words from the lemmatized tokens\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = []\n",
    "    removed_tokens = []\n",
    "    for i in tokens:\n",
    "        if i not in nltk.corpus.stopwords.words('english'):\n",
    "            filtered_tokens.append(i)\n",
    "        else:\n",
    "            removed_tokens.append(i)\n",
    "    return filtered_tokens , removed_tokens\n",
    "\n",
    "tokens_filter1, tokens_remove1 = remove_stopwords(lemmatized_tokens)\n",
    "print(\"Following are the filtered tokens: \\n\")\n",
    "print(tokens_filter1)\n",
    "print(\"\\nFollowing are the removed tokens: \\n\")\n",
    "print(tokens_remove1)\n",
    "tokens_filter2, tokens_remove2 = remove_stopwords(lemmatized_tokens2)\n",
    "print(\"\\nFollowing are the filtered tokens: \\n\")\n",
    "print(tokens_filter2)\n",
    "print(\"\\nFollowing are the removed tokens: \\n\")\n",
    "print(tokens_remove2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35918831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['field', 'astronomy', 'telescope', 'serve', 'window', 'cosmos', 'capturing', 'photon', 'emitted', 'celestial', 'body', 'million', 'light-year', 'away', 'lens', 'observe', 'intricate', 'dance', 'galaxy', 'containing', 'billion', 'star', 'held', 'together', 'gravity', \"'s\", 'unyielding', 'embrace', 'amidst', 'stellar', 'congregation', 'nebula', 'shimmer', 'birth', 'new', 'star', 'gaseous', 'cloud', 'sculpted', 'force', 'radiation', 'stellar', 'wind', 'closer', 'home', 'solar', 'system', 'teems', 'activity', 'planet', 'orbiting', 'sun', 'delicate', 'balance', 'gravitational', 'force', 'yet', 'beyond', 'familiar', 'realm', 'cosmic', 'backyard', 'lie', 'mystery', 'dark', 'matter', 'dark', 'energy', 'invisible', 'force', 'shape', 'fabric', 'universe', 'yet', 'elude', 'understanding']\n",
      "[',', ',', '.', ',', ',', '.', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.']\n",
      "['meanwhile', 'aboard', 'international', 'space', 'station', 'astronaut', 'conduct', 'experiment', 'microgravity', 'offering', 'insight', 'behavior', 'matter', 'biological', 'process', 'space', 'orbiting', 'laboratory', 'scientist', 'study', 'effect', 'long-duration', 'space', 'travel', 'human', 'body', 'essential', 'future', 'mission', 'mar', 'beyond', 'outside', 'station', 'earth', 'hang', 'suspended', 'backdrop', 'cosmos', 'reminder', 'fragility', 'existence', 'vastness', 'space', 'yet', 'also', 'beacon', 'hope', 'testament', 'humanity', \"'s\", 'ability', 'reach', 'beyond', 'confines', 'planet', 'explore', 'wonder', 'universe']\n",
      "[',', '(', ')', ',', ',', '.', ',', ',', '.', ',', ',', '.', ',', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "# Before calculating TF*IDF values, we will remove (, '' \"\" .)\n",
    "import string\n",
    "final_tokens_1 = []\n",
    "removed = []\n",
    "for i in tokens_filter1:\n",
    "    if i not in string.punctuation + \"``\" + \"''\"+ \"\" + \"()\":\n",
    "        final_tokens_1.append(i)\n",
    "    else:\n",
    "        removed.append(i)\n",
    "print(final_tokens_1)\n",
    "print(removed)\n",
    "final_tokens_2 = []\n",
    "removed2 = []\n",
    "for i in tokens_filter2:\n",
    "    if i not in string.punctuation + \"``\" + \"''\"+ \"\" + \"()\":\n",
    "        final_tokens_2.append(i)\n",
    "    else:\n",
    "        removed2.append(i)\n",
    "print(final_tokens_2)\n",
    "print(removed2)\n",
    "\n",
    "# or we can use string.isalpha() method to check that the string is alphabetical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34be09e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'field': 1, 'astronomy': 1, 'telescope': 1, 'serve': 1, 'window': 1, 'cosmos': 1, 'capturing': 1, 'photon': 1, 'emitted': 1, 'celestial': 1, 'body': 1, 'million': 1, 'light-year': 1, 'away': 1, 'lens': 1, 'observe': 1, 'intricate': 1, 'dance': 1, 'galaxy': 1, 'containing': 1, 'billion': 1, 'star': 2, 'held': 1, 'together': 1, 'gravity': 1, \"'s\": 1, 'unyielding': 1, 'embrace': 1, 'amidst': 1, 'stellar': 2, 'congregation': 1, 'nebula': 1, 'shimmer': 1, 'birth': 1, 'new': 1, 'gaseous': 1, 'cloud': 1, 'sculpted': 1, 'force': 3, 'radiation': 1, 'wind': 1, 'closer': 1, 'home': 1, 'solar': 1, 'system': 1, 'teems': 1, 'activity': 1, 'planet': 1, 'orbiting': 1, 'sun': 1, 'delicate': 1, 'balance': 1, 'gravitational': 1, 'yet': 2, 'beyond': 1, 'familiar': 1, 'realm': 1, 'cosmic': 1, 'backyard': 1, 'lie': 1, 'mystery': 1, 'dark': 2, 'matter': 1, 'energy': 1, 'invisible': 1, 'shape': 1, 'fabric': 1, 'universe': 1, 'elude': 1, 'understanding': 1}\n"
     ]
    }
   ],
   "source": [
    "# TF*IDF FREQUENCIES CALCULATION\n",
    "\n",
    "# Firstly we will calculate term frequencies using a dictionary in python\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for i in final_tokens_1:\n",
    "    if i in dict:\n",
    "        dict[i] += 1\n",
    "    else:\n",
    "        dict[i] = 1\n",
    "        \n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2033c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens =  76\n",
      "field 1\n",
      "astronomy 1\n",
      "telescope 1\n",
      "serve 1\n",
      "window 1\n",
      "cosmos 1\n",
      "capturing 1\n",
      "photon 1\n",
      "emitted 1\n",
      "celestial 1\n",
      "body 1\n",
      "million 1\n",
      "light-year 1\n",
      "away 1\n",
      "lens 1\n",
      "observe 1\n",
      "intricate 1\n",
      "dance 1\n",
      "galaxy 1\n",
      "containing 1\n",
      "billion 1\n",
      "star 2\n",
      "held 1\n",
      "together 1\n",
      "gravity 1\n",
      "'s 1\n",
      "unyielding 1\n",
      "embrace 1\n",
      "amidst 1\n",
      "stellar 2\n",
      "congregation 1\n",
      "nebula 1\n",
      "shimmer 1\n",
      "birth 1\n",
      "new 1\n",
      "gaseous 1\n",
      "cloud 1\n",
      "sculpted 1\n",
      "force 3\n",
      "radiation 1\n",
      "wind 1\n",
      "closer 1\n",
      "home 1\n",
      "solar 1\n",
      "system 1\n",
      "teems 1\n",
      "activity 1\n",
      "planet 1\n",
      "orbiting 1\n",
      "sun 1\n",
      "delicate 1\n",
      "balance 1\n",
      "gravitational 1\n",
      "yet 2\n",
      "beyond 1\n",
      "familiar 1\n",
      "realm 1\n",
      "cosmic 1\n",
      "backyard 1\n",
      "lie 1\n",
      "mystery 1\n",
      "dark 2\n",
      "matter 1\n",
      "energy 1\n",
      "invisible 1\n",
      "shape 1\n",
      "fabric 1\n",
      "universe 1\n",
      "elude 1\n",
      "understanding 1\n",
      "\n",
      "\n",
      "{'field': 0.013157894736842105, 'astronomy': 0.013157894736842105, 'telescope': 0.013157894736842105, 'serve': 0.013157894736842105, 'window': 0.013157894736842105, 'cosmos': 0.013157894736842105, 'capturing': 0.013157894736842105, 'photon': 0.013157894736842105, 'emitted': 0.013157894736842105, 'celestial': 0.013157894736842105, 'body': 0.013157894736842105, 'million': 0.013157894736842105, 'light-year': 0.013157894736842105, 'away': 0.013157894736842105, 'lens': 0.013157894736842105, 'observe': 0.013157894736842105, 'intricate': 0.013157894736842105, 'dance': 0.013157894736842105, 'galaxy': 0.013157894736842105, 'containing': 0.013157894736842105, 'billion': 0.013157894736842105, 'star': 0.02631578947368421, 'held': 0.013157894736842105, 'together': 0.013157894736842105, 'gravity': 0.013157894736842105, \"'s\": 0.013157894736842105, 'unyielding': 0.013157894736842105, 'embrace': 0.013157894736842105, 'amidst': 0.013157894736842105, 'stellar': 0.02631578947368421, 'congregation': 0.013157894736842105, 'nebula': 0.013157894736842105, 'shimmer': 0.013157894736842105, 'birth': 0.013157894736842105, 'new': 0.013157894736842105, 'gaseous': 0.013157894736842105, 'cloud': 0.013157894736842105, 'sculpted': 0.013157894736842105, 'force': 0.039473684210526314, 'radiation': 0.013157894736842105, 'wind': 0.013157894736842105, 'closer': 0.013157894736842105, 'home': 0.013157894736842105, 'solar': 0.013157894736842105, 'system': 0.013157894736842105, 'teems': 0.013157894736842105, 'activity': 0.013157894736842105, 'planet': 0.013157894736842105, 'orbiting': 0.013157894736842105, 'sun': 0.013157894736842105, 'delicate': 0.013157894736842105, 'balance': 0.013157894736842105, 'gravitational': 0.013157894736842105, 'yet': 0.02631578947368421, 'beyond': 0.013157894736842105, 'familiar': 0.013157894736842105, 'realm': 0.013157894736842105, 'cosmic': 0.013157894736842105, 'backyard': 0.013157894736842105, 'lie': 0.013157894736842105, 'mystery': 0.013157894736842105, 'dark': 0.02631578947368421, 'matter': 0.013157894736842105, 'energy': 0.013157894736842105, 'invisible': 0.013157894736842105, 'shape': 0.013157894736842105, 'fabric': 0.013157894736842105, 'universe': 0.013157894736842105, 'elude': 0.013157894736842105, 'understanding': 0.013157894736842105}\n"
     ]
    }
   ],
   "source": [
    "length_of_document = len(final_tokens_1)\n",
    "\n",
    "print(\"Total number of tokens = \", length_of_document)\n",
    "\n",
    "for word,freq in dict.items():\n",
    "    print(word,freq)\n",
    "    dict[word] = freq/length_of_document\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "497204f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanwhile 1\n",
      "aboard 1\n",
      "international 1\n",
      "space 4\n",
      "station 2\n",
      "astronaut 1\n",
      "conduct 1\n",
      "experiment 1\n",
      "microgravity 1\n",
      "offering 1\n",
      "insight 1\n",
      "behavior 1\n",
      "matter 1\n",
      "biological 1\n",
      "process 1\n",
      "orbiting 1\n",
      "laboratory 1\n",
      "scientist 1\n",
      "study 1\n",
      "effect 1\n",
      "long-duration 1\n",
      "travel 1\n",
      "human 1\n",
      "body 1\n",
      "essential 1\n",
      "future 1\n",
      "mission 1\n",
      "mar 1\n",
      "beyond 2\n",
      "outside 1\n",
      "earth 1\n",
      "hang 1\n",
      "suspended 1\n",
      "backdrop 1\n",
      "cosmos 1\n",
      "reminder 1\n",
      "fragility 1\n",
      "existence 1\n",
      "vastness 1\n",
      "yet 1\n",
      "also 1\n",
      "beacon 1\n",
      "hope 1\n",
      "testament 1\n",
      "humanity 1\n",
      "'s 1\n",
      "ability 1\n",
      "reach 1\n",
      "confines 1\n",
      "planet 1\n",
      "explore 1\n",
      "wonder 1\n",
      "universe 1\n",
      "{'meanwhile': 0.017241379310344827, 'aboard': 0.017241379310344827, 'international': 0.017241379310344827, 'space': 0.06896551724137931, 'station': 0.034482758620689655, 'astronaut': 0.017241379310344827, 'conduct': 0.017241379310344827, 'experiment': 0.017241379310344827, 'microgravity': 0.017241379310344827, 'offering': 0.017241379310344827, 'insight': 0.017241379310344827, 'behavior': 0.017241379310344827, 'matter': 0.017241379310344827, 'biological': 0.017241379310344827, 'process': 0.017241379310344827, 'orbiting': 0.017241379310344827, 'laboratory': 0.017241379310344827, 'scientist': 0.017241379310344827, 'study': 0.017241379310344827, 'effect': 0.017241379310344827, 'long-duration': 0.017241379310344827, 'travel': 0.017241379310344827, 'human': 0.017241379310344827, 'body': 0.017241379310344827, 'essential': 0.017241379310344827, 'future': 0.017241379310344827, 'mission': 0.017241379310344827, 'mar': 0.017241379310344827, 'beyond': 0.034482758620689655, 'outside': 0.017241379310344827, 'earth': 0.017241379310344827, 'hang': 0.017241379310344827, 'suspended': 0.017241379310344827, 'backdrop': 0.017241379310344827, 'cosmos': 0.017241379310344827, 'reminder': 0.017241379310344827, 'fragility': 0.017241379310344827, 'existence': 0.017241379310344827, 'vastness': 0.017241379310344827, 'yet': 0.017241379310344827, 'also': 0.017241379310344827, 'beacon': 0.017241379310344827, 'hope': 0.017241379310344827, 'testament': 0.017241379310344827, 'humanity': 0.017241379310344827, \"'s\": 0.017241379310344827, 'ability': 0.017241379310344827, 'reach': 0.017241379310344827, 'confines': 0.017241379310344827, 'planet': 0.017241379310344827, 'explore': 0.017241379310344827, 'wonder': 0.017241379310344827, 'universe': 0.017241379310344827}\n"
     ]
    }
   ],
   "source": [
    "dict2 = {}\n",
    "\n",
    "for i in final_tokens_2:\n",
    "    if i in dict2:\n",
    "        dict2[i] += 1\n",
    "    else:\n",
    "        dict2[i] = 1\n",
    "        \n",
    "length = len(final_tokens_2)\n",
    "for word,freq in dict2.items():\n",
    "    print(word,freq)\n",
    "    dict2[word] = freq/length\n",
    "    \n",
    "print(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c39aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth', 'stellar', 'meanwhile', 'million', 'space', 'outside', 'solar', 'universe', 'balance', 'new', 'delicate', 'familiar', 'reminder', 'held', 'realm', 'closer', 'reach', 'dark', 'cosmos', 'gaseous', 'explore', 'conduct', 'observe', 'invisible', 'hang', 'long-duration', 'gravitational', 'unyielding', 'astronaut', 'matter', 'intricate', 'existence', 'also', 'celestial', 'galaxy', 'teems', 'lens', 'biological', 'backdrop', 'testament', 'wonder', 'behavior', 'force', 'field', 'mar', 'amidst', 'mystery', 'insight', 'serve', 'nebula', 'light-year', 'astronomy', 'cloud', 'aboard', 'star', 'mission', 'photon', 'billion', 'telescope', 'experiment', 'orbiting', 'future', 'body', 'capturing', 'station', 'planet', 'containing', 'international', 'laboratory', 'travel', 'ability', 'congregation', 'beyond', 'human', 'earth', 'study', 'hope', 'lie', 'elude', 'sun', 'offering', 'process', 'microgravity', 'backyard', 'understanding', 'home', 'yet', 'fragility', 'wind', 'together', 'radiation', 'embrace', 'vastness', 'beacon', 'confines', 'cosmic', 'fabric', 'emitted', \"'s\", 'away', 'humanity', 'window', 'dance', 'activity', 'energy', 'essential', 'system', 'gravity', 'suspended', 'scientist', 'sculpted', 'effect', 'shape', 'shimmer'}\n",
      "{'birth': 0.3010299956639812, 'stellar': 0.3010299956639812, 'meanwhile': 0.3010299956639812, 'million': 0.3010299956639812, 'space': 0.3010299956639812, 'outside': 0.3010299956639812, 'solar': 0.3010299956639812, 'universe': 0.0, 'balance': 0.3010299956639812, 'new': 0.3010299956639812, 'delicate': 0.3010299956639812, 'familiar': 0.3010299956639812, 'reminder': 0.3010299956639812, 'held': 0.3010299956639812, 'realm': 0.3010299956639812, 'closer': 0.3010299956639812, 'reach': 0.3010299956639812, 'dark': 0.3010299956639812, 'cosmos': 0.0, 'gaseous': 0.3010299956639812, 'explore': 0.3010299956639812, 'conduct': 0.3010299956639812, 'observe': 0.3010299956639812, 'invisible': 0.3010299956639812, 'hang': 0.3010299956639812, 'long-duration': 0.3010299956639812, 'gravitational': 0.3010299956639812, 'unyielding': 0.3010299956639812, 'astronaut': 0.3010299956639812, 'matter': 0.0, 'intricate': 0.3010299956639812, 'existence': 0.3010299956639812, 'also': 0.3010299956639812, 'celestial': 0.3010299956639812, 'galaxy': 0.3010299956639812, 'teems': 0.3010299956639812, 'lens': 0.3010299956639812, 'biological': 0.3010299956639812, 'backdrop': 0.3010299956639812, 'testament': 0.3010299956639812, 'wonder': 0.3010299956639812, 'behavior': 0.3010299956639812, 'force': 0.3010299956639812, 'field': 0.3010299956639812, 'mar': 0.3010299956639812, 'amidst': 0.3010299956639812, 'mystery': 0.3010299956639812, 'insight': 0.3010299956639812, 'serve': 0.3010299956639812, 'nebula': 0.3010299956639812, 'light-year': 0.3010299956639812, 'astronomy': 0.3010299956639812, 'cloud': 0.3010299956639812, 'aboard': 0.3010299956639812, 'star': 0.3010299956639812, 'mission': 0.3010299956639812, 'photon': 0.3010299956639812, 'billion': 0.3010299956639812, 'telescope': 0.3010299956639812, 'experiment': 0.3010299956639812, 'orbiting': 0.0, 'future': 0.3010299956639812, 'body': 0.0, 'capturing': 0.3010299956639812, 'station': 0.3010299956639812, 'planet': 0.0, 'containing': 0.3010299956639812, 'international': 0.3010299956639812, 'laboratory': 0.3010299956639812, 'travel': 0.3010299956639812, 'ability': 0.3010299956639812, 'congregation': 0.3010299956639812, 'beyond': 0.0, 'human': 0.3010299956639812, 'earth': 0.3010299956639812, 'study': 0.3010299956639812, 'hope': 0.3010299956639812, 'lie': 0.3010299956639812, 'elude': 0.3010299956639812, 'sun': 0.3010299956639812, 'offering': 0.3010299956639812, 'process': 0.3010299956639812, 'microgravity': 0.3010299956639812, 'backyard': 0.3010299956639812, 'understanding': 0.3010299956639812, 'home': 0.3010299956639812, 'yet': 0.0, 'fragility': 0.3010299956639812, 'wind': 0.3010299956639812, 'together': 0.3010299956639812, 'radiation': 0.3010299956639812, 'embrace': 0.3010299956639812, 'vastness': 0.3010299956639812, 'beacon': 0.3010299956639812, 'confines': 0.3010299956639812, 'cosmic': 0.3010299956639812, 'fabric': 0.3010299956639812, 'emitted': 0.3010299956639812, \"'s\": 0.0, 'away': 0.3010299956639812, 'humanity': 0.3010299956639812, 'window': 0.3010299956639812, 'dance': 0.3010299956639812, 'activity': 0.3010299956639812, 'energy': 0.3010299956639812, 'essential': 0.3010299956639812, 'system': 0.3010299956639812, 'gravity': 0.3010299956639812, 'suspended': 0.3010299956639812, 'scientist': 0.3010299956639812, 'sculpted': 0.3010299956639812, 'effect': 0.3010299956639812, 'shape': 0.3010299956639812, 'shimmer': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "# Calculating IDF values\n",
    "import math\n",
    "uniqueWords = set()\n",
    "def calculate_idf(docList):\n",
    "    idf = {}\n",
    "    for dict in docList:\n",
    "        for word in dict:\n",
    "            uniqueWords.add(word)\n",
    "            \n",
    "    print(uniqueWords)\n",
    "    \n",
    "    for word in uniqueWords:\n",
    "        for dict in docList:\n",
    "            if word in dict:\n",
    "                if word in idf:\n",
    "                    idf[word] += 1\n",
    "                else:\n",
    "                    idf[word] = 1\n",
    "    # check all the unique words and all the dictionaries if the word is in the dictionary and in the idf then do idf[word] += 1\n",
    "    # else do idf[word] = 1\n",
    "    # basically it calculates how many files have a particular word and the word is taken from uniqueWords set\n",
    "    \n",
    "    # NOW WE HAVE TO CALCULATE IDF USING THE FORMULA\n",
    "    # formula is total number of documents/ documents which contain the particular word\n",
    "    for word, values in idf.items():\n",
    "        idf[word] = math.log10(len(docList)/idf[word])\n",
    "                    \n",
    "    return idf\n",
    "\n",
    "idf_dict = calculate_idf([dict,dict2])\n",
    "print(idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1cd43a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'field': 0.0039609209955787, 'astronomy': 0.0039609209955787, 'telescope': 0.0039609209955787, 'serve': 0.0039609209955787, 'window': 0.0039609209955787, 'cosmos': 0.0, 'capturing': 0.0039609209955787, 'photon': 0.0039609209955787, 'emitted': 0.0039609209955787, 'celestial': 0.0039609209955787, 'body': 0.0, 'million': 0.0039609209955787, 'light-year': 0.0039609209955787, 'away': 0.0039609209955787, 'lens': 0.0039609209955787, 'observe': 0.0039609209955787, 'intricate': 0.0039609209955787, 'dance': 0.0039609209955787, 'galaxy': 0.0039609209955787, 'containing': 0.0039609209955787, 'billion': 0.0039609209955787, 'star': 0.0079218419911574, 'held': 0.0039609209955787, 'together': 0.0039609209955787, 'gravity': 0.0039609209955787, \"'s\": 0.0, 'unyielding': 0.0039609209955787, 'embrace': 0.0039609209955787, 'amidst': 0.0039609209955787, 'stellar': 0.0079218419911574, 'congregation': 0.0039609209955787, 'nebula': 0.0039609209955787, 'shimmer': 0.0039609209955787, 'birth': 0.0039609209955787, 'new': 0.0039609209955787, 'gaseous': 0.0039609209955787, 'cloud': 0.0039609209955787, 'sculpted': 0.0039609209955787, 'force': 0.011882762986736099, 'radiation': 0.0039609209955787, 'wind': 0.0039609209955787, 'closer': 0.0039609209955787, 'home': 0.0039609209955787, 'solar': 0.0039609209955787, 'system': 0.0039609209955787, 'teems': 0.0039609209955787, 'activity': 0.0039609209955787, 'planet': 0.0, 'orbiting': 0.0, 'sun': 0.0039609209955787, 'delicate': 0.0039609209955787, 'balance': 0.0039609209955787, 'gravitational': 0.0039609209955787, 'yet': 0.0, 'beyond': 0.0, 'familiar': 0.0039609209955787, 'realm': 0.0039609209955787, 'cosmic': 0.0039609209955787, 'backyard': 0.0039609209955787, 'lie': 0.0039609209955787, 'mystery': 0.0039609209955787, 'dark': 0.0079218419911574, 'matter': 0.0, 'energy': 0.0039609209955787, 'invisible': 0.0039609209955787, 'shape': 0.0039609209955787, 'fabric': 0.0039609209955787, 'universe': 0.0, 'elude': 0.0039609209955787, 'understanding': 0.0039609209955787}\n",
      "\n",
      "{'meanwhile': 0.005190172339034158, 'aboard': 0.005190172339034158, 'international': 0.005190172339034158, 'space': 0.020760689356136633, 'station': 0.010380344678068316, 'astronaut': 0.005190172339034158, 'conduct': 0.005190172339034158, 'experiment': 0.005190172339034158, 'microgravity': 0.005190172339034158, 'offering': 0.005190172339034158, 'insight': 0.005190172339034158, 'behavior': 0.005190172339034158, 'matter': 0.0, 'biological': 0.005190172339034158, 'process': 0.005190172339034158, 'orbiting': 0.0, 'laboratory': 0.005190172339034158, 'scientist': 0.005190172339034158, 'study': 0.005190172339034158, 'effect': 0.005190172339034158, 'long-duration': 0.005190172339034158, 'travel': 0.005190172339034158, 'human': 0.005190172339034158, 'body': 0.0, 'essential': 0.005190172339034158, 'future': 0.005190172339034158, 'mission': 0.005190172339034158, 'mar': 0.005190172339034158, 'beyond': 0.0, 'outside': 0.005190172339034158, 'earth': 0.005190172339034158, 'hang': 0.005190172339034158, 'suspended': 0.005190172339034158, 'backdrop': 0.005190172339034158, 'cosmos': 0.0, 'reminder': 0.005190172339034158, 'fragility': 0.005190172339034158, 'existence': 0.005190172339034158, 'vastness': 0.005190172339034158, 'yet': 0.0, 'also': 0.005190172339034158, 'beacon': 0.005190172339034158, 'hope': 0.005190172339034158, 'testament': 0.005190172339034158, 'humanity': 0.005190172339034158, \"'s\": 0.0, 'ability': 0.005190172339034158, 'reach': 0.005190172339034158, 'confines': 0.005190172339034158, 'planet': 0.0, 'explore': 0.005190172339034158, 'wonder': 0.005190172339034158, 'universe': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# now multiply idf values with tf values\n",
    "\n",
    "tf_idf1 = {}\n",
    "tf1 = dict\n",
    "tf2 = dict2\n",
    "\n",
    "for word in tf1.keys():\n",
    "    tf_idf1[word] = tf1[word] * idf_dict[word]\n",
    "    \n",
    "tf_idf2 = {}\n",
    "\n",
    "for word in tf2.keys():\n",
    "    tf_idf2[word] = tf2[word] * idf_dict[word]\n",
    "    \n",
    "print()\n",
    "print(tf_idf1)\n",
    "print()\n",
    "print(tf_idf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac95577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c313f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56aa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804be73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
